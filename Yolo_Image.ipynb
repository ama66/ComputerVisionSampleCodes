{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10a26065-392a-4e5d-8629-1025b74d65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5696b7fa-9c94-4488-b4de-aa8f237263ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_objects(model_outputs):\n",
    "    bounding_box_locations = []\n",
    "    class_ids = []\n",
    "    confidence_values = []\n",
    "\n",
    "    for output in model_outputs:\n",
    "        for prediction in output:\n",
    "            ## find class id with the highest level of probability/confidence \n",
    "            class_probabilities = prediction[5:]\n",
    "            class_id = np.argmax(class_probabilities)\n",
    "            confidence = class_probabilities[class_id]\n",
    "\n",
    "            if confidence > THRESHOLD:\n",
    "                w, h = int(prediction[2] * YOLO_IMAGE_SIZE), int(prediction[3] * YOLO_IMAGE_SIZE)\n",
    "                ## rescale output to blob dimensions! \n",
    "                # the center of the bounding box (we should transform these values)\n",
    "                x, y = int(prediction[0] * YOLO_IMAGE_SIZE - w / 2), int(prediction[1] * YOLO_IMAGE_SIZE - h / 2)\n",
    "                ## now x,y are the coordinates of top left corner of the bounding box \n",
    "                bounding_box_locations.append([x, y, w, h])\n",
    "                class_ids.append(class_id)\n",
    "                confidence_values.append(float(confidence))\n",
    "\n",
    "    box_indexes_to_keep = cv2.dnn.NMSBoxes(bounding_box_locations, confidence_values, THRESHOLD, SUPPRESSION_THRESHOLD)\n",
    "\n",
    "    return box_indexes_to_keep, bounding_box_locations, class_ids, confidence_values\n",
    "\n",
    "\n",
    "def show_detected_images(img, bounding_box_ids, all_bounding_boxes, class_ids, confidence_values, width_ratio,\n",
    "                         height_ratio):\n",
    "    for index in bounding_box_ids:\n",
    "        bounding_box = all_bounding_boxes[index[0]]\n",
    "        x, y, w, h = int(bounding_box[0]), int(bounding_box[1]), int(bounding_box[2]), int(bounding_box[3])\n",
    "        # we have to transform the locations adn coordinates because the resized image\n",
    "        x = int(x*width_ratio)\n",
    "        y = int(y * height_ratio)\n",
    "        w = int(w * width_ratio)\n",
    "        h = int(h * height_ratio)\n",
    "\n",
    "        # OpenCV deals with BGR blue green red (255,0,0) then it is the blue color\n",
    "        # we are not going to detect every objects just PERSON and CAR\n",
    "        if class_ids[index[0]] == 2:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            class_with_confidence = 'CAR' + str(int(confidence_values[index[0]] * 100)) + '%'\n",
    "            cv2.putText(img, class_with_confidence, (x, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "        if class_ids[index[0]] == 0:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            class_with_confidence = 'PERSON' + str(int(confidence_values[index[0]] * 100)) + '%'\n",
    "            cv2.putText(img, class_with_confidence, (x, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c07509d-55e1-4115-89c1-c80ebe7afa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are not going to bother with objects less than 30% probability\n",
    "THRESHOLD = 0.3\n",
    "# the lower the value: the fewer bounding boxes will remain\n",
    "SUPPRESSION_THRESHOLD = 0.3\n",
    "YOLO_IMAGE_SIZE = 320\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a55b698-56a2-4e07-967b-640c2fc8c36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread('camus.jpg')\n",
    "original_width, original_height = image.shape[1], image.shape[0]\n",
    "\n",
    "# there are 80 (90) possible output classes\n",
    "# 0: person - 2: car - 5: bus\n",
    "classes = ['car', 'person', 'bus']\n",
    "\n",
    "neural_network = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\n",
    "# define whether we run the algorithm with CPU or with GPU\n",
    "# WE ARE GOING TO USE CPU !!!\n",
    "neural_network.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "neural_network.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# the image into a BLOB [0-1] RGB - BGR\n",
    "blob = cv2.dnn.blobFromImage(image, 1 / 255, (YOLO_IMAGE_SIZE, YOLO_IMAGE_SIZE), True, crop=False)\n",
    "neural_network.setInput(blob)\n",
    "\n",
    "## The entire network architecture (list of names)\n",
    "layer_names = neural_network.getLayerNames()\n",
    "# call getunconnectedoutlayers to find out the indices of the yolo output layer names\n",
    "\n",
    "# YOLO network has 3 output layer - note: these indexes are starting with 1\n",
    "output_names = [layer_names[index[0] - 1] for index in neural_network.getUnconnectedOutLayers()]\n",
    "\n",
    "# Generate output prediction from all three yolo output layers \n",
    "## by specifying their names  \n",
    "\n",
    "outputs = neural_network.forward(output_names)\n",
    "\n",
    "## dimensions of output list # ((300, 85), (1200, 85), (4800, 85)) \n",
    "## have 300 bounding boxes from first layer, 1200 boxes from second layer, \n",
    "## and 4800 bounding boxes from third layer \n",
    "## for each bounding box we have a prediction vector of dimension = 85 \n",
    "## (x,y,w,h, confidence, 80 class probabilities)\n",
    "\n",
    "# Massage the output and preprocess them to be ready for the show detected image function\n",
    "\n",
    "\n",
    "predicted_objects, bbox_locations, class_label_ids, conf_values = find_objects(outputs)\n",
    "\n",
    "## suppress non-max and find the objects with their corresponding bounding boxes \n",
    "\n",
    "show_detected_images(image, predicted_objects, bbox_locations, class_label_ids, conf_values,\n",
    "                     original_width / YOLO_IMAGE_SIZE, original_height / YOLO_IMAGE_SIZE)\n",
    "\n",
    "cv2.imshow('YOLO Algorithm', image)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee25ce4a-1899-4d8f-8444-d1af814abac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv_0',\n",
       " 'bn_0',\n",
       " 'leaky_1',\n",
       " 'conv_1',\n",
       " 'bn_1',\n",
       " 'leaky_2',\n",
       " 'conv_2',\n",
       " 'bn_2',\n",
       " 'leaky_3',\n",
       " 'conv_3',\n",
       " 'bn_3',\n",
       " 'leaky_4',\n",
       " 'shortcut_4',\n",
       " 'conv_5',\n",
       " 'bn_5',\n",
       " 'leaky_6',\n",
       " 'conv_6',\n",
       " 'bn_6',\n",
       " 'leaky_7',\n",
       " 'conv_7',\n",
       " 'bn_7',\n",
       " 'leaky_8',\n",
       " 'shortcut_8',\n",
       " 'conv_9',\n",
       " 'bn_9',\n",
       " 'leaky_10',\n",
       " 'conv_10',\n",
       " 'bn_10',\n",
       " 'leaky_11',\n",
       " 'shortcut_11',\n",
       " 'conv_12',\n",
       " 'bn_12',\n",
       " 'leaky_13',\n",
       " 'conv_13',\n",
       " 'bn_13',\n",
       " 'leaky_14',\n",
       " 'conv_14',\n",
       " 'bn_14',\n",
       " 'leaky_15',\n",
       " 'shortcut_15',\n",
       " 'conv_16',\n",
       " 'bn_16',\n",
       " 'leaky_17',\n",
       " 'conv_17',\n",
       " 'bn_17',\n",
       " 'leaky_18',\n",
       " 'shortcut_18',\n",
       " 'conv_19',\n",
       " 'bn_19',\n",
       " 'leaky_20',\n",
       " 'conv_20',\n",
       " 'bn_20',\n",
       " 'leaky_21',\n",
       " 'shortcut_21',\n",
       " 'conv_22',\n",
       " 'bn_22',\n",
       " 'leaky_23',\n",
       " 'conv_23',\n",
       " 'bn_23',\n",
       " 'leaky_24',\n",
       " 'shortcut_24',\n",
       " 'conv_25',\n",
       " 'bn_25',\n",
       " 'leaky_26',\n",
       " 'conv_26',\n",
       " 'bn_26',\n",
       " 'leaky_27',\n",
       " 'shortcut_27',\n",
       " 'conv_28',\n",
       " 'bn_28',\n",
       " 'leaky_29',\n",
       " 'conv_29',\n",
       " 'bn_29',\n",
       " 'leaky_30',\n",
       " 'shortcut_30',\n",
       " 'conv_31',\n",
       " 'bn_31',\n",
       " 'leaky_32',\n",
       " 'conv_32',\n",
       " 'bn_32',\n",
       " 'leaky_33',\n",
       " 'shortcut_33',\n",
       " 'conv_34',\n",
       " 'bn_34',\n",
       " 'leaky_35',\n",
       " 'conv_35',\n",
       " 'bn_35',\n",
       " 'leaky_36',\n",
       " 'shortcut_36',\n",
       " 'conv_37',\n",
       " 'bn_37',\n",
       " 'leaky_38',\n",
       " 'conv_38',\n",
       " 'bn_38',\n",
       " 'leaky_39',\n",
       " 'conv_39',\n",
       " 'bn_39',\n",
       " 'leaky_40',\n",
       " 'shortcut_40',\n",
       " 'conv_41',\n",
       " 'bn_41',\n",
       " 'leaky_42',\n",
       " 'conv_42',\n",
       " 'bn_42',\n",
       " 'leaky_43',\n",
       " 'shortcut_43',\n",
       " 'conv_44',\n",
       " 'bn_44',\n",
       " 'leaky_45',\n",
       " 'conv_45',\n",
       " 'bn_45',\n",
       " 'leaky_46',\n",
       " 'shortcut_46',\n",
       " 'conv_47',\n",
       " 'bn_47',\n",
       " 'leaky_48',\n",
       " 'conv_48',\n",
       " 'bn_48',\n",
       " 'leaky_49',\n",
       " 'shortcut_49',\n",
       " 'conv_50',\n",
       " 'bn_50',\n",
       " 'leaky_51',\n",
       " 'conv_51',\n",
       " 'bn_51',\n",
       " 'leaky_52',\n",
       " 'shortcut_52',\n",
       " 'conv_53',\n",
       " 'bn_53',\n",
       " 'leaky_54',\n",
       " 'conv_54',\n",
       " 'bn_54',\n",
       " 'leaky_55',\n",
       " 'shortcut_55',\n",
       " 'conv_56',\n",
       " 'bn_56',\n",
       " 'leaky_57',\n",
       " 'conv_57',\n",
       " 'bn_57',\n",
       " 'leaky_58',\n",
       " 'shortcut_58',\n",
       " 'conv_59',\n",
       " 'bn_59',\n",
       " 'leaky_60',\n",
       " 'conv_60',\n",
       " 'bn_60',\n",
       " 'leaky_61',\n",
       " 'shortcut_61',\n",
       " 'conv_62',\n",
       " 'bn_62',\n",
       " 'leaky_63',\n",
       " 'conv_63',\n",
       " 'bn_63',\n",
       " 'leaky_64',\n",
       " 'conv_64',\n",
       " 'bn_64',\n",
       " 'leaky_65',\n",
       " 'shortcut_65',\n",
       " 'conv_66',\n",
       " 'bn_66',\n",
       " 'leaky_67',\n",
       " 'conv_67',\n",
       " 'bn_67',\n",
       " 'leaky_68',\n",
       " 'shortcut_68',\n",
       " 'conv_69',\n",
       " 'bn_69',\n",
       " 'leaky_70',\n",
       " 'conv_70',\n",
       " 'bn_70',\n",
       " 'leaky_71',\n",
       " 'shortcut_71',\n",
       " 'conv_72',\n",
       " 'bn_72',\n",
       " 'leaky_73',\n",
       " 'conv_73',\n",
       " 'bn_73',\n",
       " 'leaky_74',\n",
       " 'shortcut_74',\n",
       " 'conv_75',\n",
       " 'bn_75',\n",
       " 'leaky_76',\n",
       " 'conv_76',\n",
       " 'bn_76',\n",
       " 'leaky_77',\n",
       " 'conv_77',\n",
       " 'bn_77',\n",
       " 'leaky_78',\n",
       " 'conv_78',\n",
       " 'bn_78',\n",
       " 'leaky_79',\n",
       " 'conv_79',\n",
       " 'bn_79',\n",
       " 'leaky_80',\n",
       " 'conv_80',\n",
       " 'bn_80',\n",
       " 'leaky_81',\n",
       " 'conv_81',\n",
       " 'permute_82',\n",
       " 'yolo_82',\n",
       " 'identity_83',\n",
       " 'conv_84',\n",
       " 'bn_84',\n",
       " 'leaky_85',\n",
       " 'upsample_85',\n",
       " 'concat_86',\n",
       " 'conv_87',\n",
       " 'bn_87',\n",
       " 'leaky_88',\n",
       " 'conv_88',\n",
       " 'bn_88',\n",
       " 'leaky_89',\n",
       " 'conv_89',\n",
       " 'bn_89',\n",
       " 'leaky_90',\n",
       " 'conv_90',\n",
       " 'bn_90',\n",
       " 'leaky_91',\n",
       " 'conv_91',\n",
       " 'bn_91',\n",
       " 'leaky_92',\n",
       " 'conv_92',\n",
       " 'bn_92',\n",
       " 'leaky_93',\n",
       " 'conv_93',\n",
       " 'permute_94',\n",
       " 'yolo_94',\n",
       " 'identity_95',\n",
       " 'conv_96',\n",
       " 'bn_96',\n",
       " 'leaky_97',\n",
       " 'upsample_97',\n",
       " 'concat_98',\n",
       " 'conv_99',\n",
       " 'bn_99',\n",
       " 'leaky_100',\n",
       " 'conv_100',\n",
       " 'bn_100',\n",
       " 'leaky_101',\n",
       " 'conv_101',\n",
       " 'bn_101',\n",
       " 'leaky_102',\n",
       " 'conv_102',\n",
       " 'bn_102',\n",
       " 'leaky_103',\n",
       " 'conv_103',\n",
       " 'bn_103',\n",
       " 'leaky_104',\n",
       " 'conv_104',\n",
       " 'bn_104',\n",
       " 'leaky_105',\n",
       " 'conv_105',\n",
       " 'permute_106',\n",
       " 'yolo_106']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the entire architecture\n",
    "layer_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1547c-1e14-4990-bc0f-868faf8079e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3be447d0-0bd4-4a91-bb61-810dd00fc025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200],\n",
       "       [227],\n",
       "       [254]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.getUnconnectedOutLayers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc50754e-db1d-4be0-a85a-4596f56ad3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yolo_82', 'yolo_94', 'yolo_106']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yolo layer names\n",
    "output_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3d25ad5-495e-4b20-9e31-694a8375a06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 85), (1200, 85), (4800, 85))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape , outputs[1].shape , outputs[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcdaa188-0640-499d-a527-d6bce9a278ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3]], dtype=int32),)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_objects, ## only fourth box contain an object! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ae42f53-6c56-4fc2-b518-23a653295e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 21, 176, 270],\n",
       " [8, 23, 183, 264],\n",
       " [-13, -1, 240, 309],\n",
       " [3, 22, 173, 291],\n",
       " [-11, 12, 204, 312],\n",
       " [5, 28, 187, 284],\n",
       " [-4, 21, 219, 298]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " bbox_locations  ## [x,y,w,h] = [3, 22, 173, 291],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672576e-1780-4598-85ca-c5c7460e55e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
